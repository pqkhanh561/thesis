\documentclass[12pt,a4paper]{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[utf8]{vietnam}
\usepackage{blindtext}
\usepackage{listings}

\usepackage{amsmath,amsthm,amssymb}
\usepackage[utf8]{vietnam}
\usepackage{blindtext}
\usepackage{listings}

\usepackage{graphicx}
\usepackage[numbers]{natbib}
\usepackage{enumitem}

\renewcommand{\baselinestretch}{1.5}
\title{Chương 5: Phương pháp Monte Carlo}
\begin{document}
\maketitle
\section{Monte Carlo Prediction}
Chúng ta sử dụng phương pháp Monte Carlo để ước lượng state-value khi biết policy. Mỗi lần quan sát được một state nhất định, việc tính trung bình mỗi lần thăm sẽ ước lượng được state-value. Hiển nhiên, khi một state được thăm nhiều lần sẽ hội tụ về giá trị kỳ vọng của state-value.\\
Cụ thể hơn, phương pháp Monte Carlo được chi làm 2 loại First-visit MC và Every-visit MC.
\section{Monte Carlo Estimate of Action Values}
Khi không có model, chỉ có state-value thôi không đủ hiệu quả[cần dẫn chứng] để xác định được policy do đó kết hợp giữa state-action value sẽ trở nên tốt hơn. Tức chúng ta tính $q{\pi}(s,a)$ thay vì $q{\pi}(s)$\\
**Chú ý** rằng model được dùng duy nhất để tạo xác suất chuyển từ các state với nhau. Tuy nhiên không cần tính hết mọi xác suất chuyển đổi tất cả các state, phương pháp Monte Carlo chỉ cần tính xác suất chuyển đổi lận cận.\\
Vì không tính hết mọi xác suất nên sẽ tồn tại các state-action có thể không được thăm. Do đó không xác định được giá trị trung bình của action, dẫn đến không có kinh nghiệm trong việc ước lượng action-value của state đó gần như vô nghĩa. Đây là vấn đề nghiệm trọng vì việc học action-value là để chọn một action trong số các action của state đó.\\
**Cần đọc GPI trong phần 4.6\\
\section{Monte Carlo Control}
\bold{Câu hỏi:} Tại sao xuất hiện exploring start?

\end{document}