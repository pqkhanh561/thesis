\documentclass[12pt,a4paper]{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[utf8]{vietnam}
\usepackage{blindtext}
\usepackage{listings}
\renewcommand{\baselinestretch}{1.5}
\title{Finite Markov Processes}
\author{Phan Quang Khánh - 1611125}
\begin{document}
\maketitle
\section{The Agent-Environment Interface}
Định nghĩa quá trình một agent tác động đến môi trường xung quanh tại thời điểm nào đó. Thường hoạt động của agent sẽ là một chuỗi các hoạt động và các môi trường cũng như penaty.
\[S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,...S_t,A_t,R_{t+1}\]
Trong finite MDP, thì các S,A,R là finite do đó ta có thể định nghĩa xác suất của $S_t,R_t$ trong một tập xác suất rời rạc. Xác suất xảy ra $s', r$ khi biết các giá trị state và action trước đó là
\[p(s',r|s,a) = Pr\{S_t = s', R_t = r|S_{t-1} = s, A_{t-1} = a\}\]
Ta cũng có công thức tính reward thông qua state và action
\[r(s,a) = E[R_t| S_{t-1} = s, A_{t-1} = a] = \sum_{r \in R}r\sum_{s'\in S}p(s',r|s,a)\]
Nhưng nếu cụ thể hơn muốn tính reward cho trạng thái s'
\[r(s,a,s') = E[S_t = s',R_t|S_{t-1} = s, A_{t-1} = a] = \sum_{r \in R}r\dfrac{p(s',r|s,a)}{p(s'|s,a)}\]
Những ký hiệu trên thường được sử dụng nhưng trong thực hành thì cách viết 4 biến sẽ được chuộng hơn. Tuy nhiên các cách viết khác cũng tiện trong một vài trường hợp.\\
\\
MDP framework có thể mô tả nhiểu bài toán trong reinforce learning, tuy nhiên không dừng lại ở đó, nó thể hiện trừu tượng hóa hơn cũng như linh động trong nhiều tình huống khác nhau. Ví dụ time không cần thể hiện như là một khoảng thời gian nhất định mà có thể biểu diễn dưới dạng từng phần riêng biệt giai đoạn tùy theo bài toán. Ngoài ra, Actions cũng có các kiểu khác nhau, như low-level và high-level. Tương tự với State và Reward.\\
\\
Chúng ta nói về cách định nghĩa môi trường và agent. Ví dụ tình trạng cơ thể có thể giả định là ở trong người nhưng thường được xem là môi trường tác động đển agent. Do đó có luật để xác định, những thứ mà agent không thể thay đổi ngẫu nhiên bởi agent được xem là môi trường.\\
\section{Goal and Rewards}
Phần này, ta định nghĩa goal và reward được xem như làm hàm heuristic để agent tìm kiếm đúng trong môi trường tìm kiếm.
\section{Returns and Episodes}
\textbf{Returns} được xem là khoảng thu lại trong một khoảng thời gian nhất định. Tức là tổng của các reward theo thời gian.
\[G_t = R_{t+1} + R_{t+2} + ... + R_T\]
Với T ở đây được xem là thời gian mà chúng ta kết thúc quá trình tìm kiếm.\\
Trong RL, trong việc tìm kiếm giá trị lớn nhất thì khoảng thời gian để tìm kiếm thường có 2 trường hợp như sau:\\
\textbf{Episodes} là các tập mà chúng tồn tại trạng thái kết thúc trong đó và các tập độc lập với nhau. Ví dụ là một ván cờ.\\
\textbf{Continous} là một quá trình nhưng không có trạng thái kết thúc. Ví dụ di chuyển robot đi khắp nơi.\\
\textbf{Công thức quan trọng:}
\begin{align*}
G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots\\
&= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots)\\
&= R_{t+1} + \gamma G_{t+1}
\end{align*}
\section{Unified Notation for Esposidic and Continuing Tasks}
Trong phần này chúng ta lập nên một ký hiệu có thể dùng chung cho môi trường mà the eposide và continuing. Mục đích để làm đơn giản hóa trong việc lập công thức.\\ 
Nếu xét môi trường eposide thì ta phải có thêm một kí hiệu khác vì mỗi eposide cần phân biệt nhau. Vậy phải có tập các ký hiệu $S_{t,i}, A_{t,i},..$ trong đó i là chỉ số của mỗi eposide. Sở dĩ có chuyện này vì có thể mỗi eposide agent gặp sẽ có các hành động khác nhau và có kêt thúc khác nhau (terminal state). Nhưng vì mục đích đơn giản hóa thì ta sẽ coi các eposide như nhau và giải quyết bài toán đúng cho tất cả các eposide. \\
Ta phải xây dựng sự xuất hiện của terminal state trong việc kí hiệu trên, do đó ta định nghĩa các trạng thái sau khi terminal xuất hiện thì $R_t = 0$
\section{Policies and Value Functions}
\textbf{Policy} là một ánh xạ từ tập states vào tập xác suất của việc chọn mỗi action có thể có. Nếu agent theo một policy $\pi$ tại thời điểm t thì $\pi(a|s)$ là xác suất mà $A_t = a$ khi $S_t = s$. Có thể xem policy là chiến thuật để tìm thuật toán tối ưu. Ví dụ giả sử con robot dọn dẹp.\\
+ Policy #1 sẽ là robot đi vòng vòng (dummy robot)\\
+ Policy #2 là robot đi đến khi gặp bức tường rồi đi theo zigzac (better robot)\\
+ Policy #3 là robot định hình được rác ở đâu và đi thẳng tới đó (smart robot)\\
\\
Vì có nhiều policy nên cần đánh giá xem policy đó có tốt không nên các hàm dưới đây sẽ đánh giá để chọn policy đó\\
\textbf{State-value function} $v_\pi(s)$ là hàm giá trị của một state s theo một policy $\pi$. Tức là giá trị mà trạng thái có được nếu áp dụng một policy nào đó.
\[v_\pi(s) = \mathbb{E}_\pi[G_t|S_t = s]\]
\textbf{Action-value function} $q_\pi(s,a)$ là hàm giá trị của action được chọn theo policy $\pi$.
\[q_\pi(s,a) = E_\pi[G_t|S_t=s, A_t = a] = E_\pi\left[\sum_{k=0}^\infty
-\gamma^K R_{t+k+1}| S_t = s, A_t = a \right]\]
\textbf{Bellman equation}
\begin{align*}
    v_{\pi}(s) &= \mathbb{E}_\pi[G_t|S_t=s]\\
    &= \mathbb{E}_\pi[R_t +\gamma G_{t+1}| S_t = t]\\
    &=\sum_{\pi}\pi(s,a)\sum_{s'}\sum_{r} p(s',r|s,a)\left[r+ \gamma E_{\pi}[G_{t+1}| S_{t+1} = s']\right]\\
    &= \sum_{\pi}\pi(s,a)\sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')]
\end{align*}
Ở dấu bằng thứ 3 xảy ra thì chúng ta sử dụng công thức xác suất đầy đủ để thu được phương trình trên.
Phương trình trên cho thấy sự phụ thuộc giữa các giá trị của state, vì vậy tính hồi quy trở nên rất quan trọng trong bài toán Reinforcement Learning.
\section{Optimal Policies and Optimal Value Functions}
Optimal Policies được xác định bằng cách chọn $\pi$ sao cho $v_\pi(s)$ lớn nhất.Ta có thể định nghĩa các giá trị cần thiết như sau:
\begin{equation*}
    v_*(s) = \max_{\pi}v_\pi (s) \text{  và  }q_*(s,a) = \max_\pi q_\pi(s,a)
\end{equation*}
Chúng ta có thể suy ra từ công thức trên được biểu thức
\begin{align*}
    v_*(s) & = \max_{\pi}\mathbb{E}_\pi[G_t|S_t = s] \\
    &=\max_{\pi}\sum_{a}\mathbb{P}(A_t = a|S_t=s)\mathbb{E}_\pi[G_t|S_t = s,A_t=a]\\
    &=\max_{\pi}\sum_{a}\pi(s,a)\mathbb{E}_\pi[G_t|S_t = s,A_t=a]\\
    &=\max_{\pi}\sum_{a}\pi(s,a)q_\pi(s,a)\\
    &=\max_{\pi}q_{\pi_*}(s,a)
\end{align*}
Nếu xác định một policy là tối ưu thì chúng ta có dấu bằng cuối cùng ở trên..
\end{document}